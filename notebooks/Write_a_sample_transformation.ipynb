{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL-Augmenter  ü¶é ‚Üí üêç Write a sample transformation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ebe94b3466943ddba5b8ab874daf900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a905fb867c8a44338d5f35f38a19b46e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14bb03f053f64a278e3968840eb959fd",
              "IPY_MODEL_0f6a1bf4be7b42fd9c0c3929fd478e3b"
            ]
          }
        },
        "a905fb867c8a44338d5f35f38a19b46e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14bb03f053f64a278e3968840eb959fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_962f4cd8298940368632450846bca967",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 891737400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 891737400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90f2d6ac1f17408a9408d6c316a1e29e"
          }
        },
        "0f6a1bf4be7b42fd9c0c3929fd478e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4958a0e991d049339723fe494f573229",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 892M/892M [00:21&lt;00:00, 42.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8933b6543ead40fb826e135d82f5982c"
          }
        },
        "962f4cd8298940368632450846bca967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90f2d6ac1f17408a9408d6c316a1e29e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4958a0e991d049339723fe494f573229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8933b6543ead40fb826e135d82f5982c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drfP193NQSZ6"
      },
      "source": [
        "<a href =\"https://colab.research.google.com/github/GEM-benchmark/NL-Augmenter/blob/main/notebooks/Write_a_sample_transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMrWwwGdP4wO"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QcAG8bThqq"
      },
      "source": [
        "# NL-Augmenter Colab example \n",
        "\n",
        "  * Play with an existing **transformation** \n",
        "    * Write your own **transformation** \n",
        "  * Play with an existing **filter**  \n",
        "    * Write your own **filter**         \n",
        "\n",
        "Total running time: ~10 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_4yHCIAvQx"
      },
      "source": [
        "## Install NL-Augmenter from GitHub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkv4WSJsI7YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c69b50-9c23-4794-85da-087e641a3b99"
      },
      "source": [
        "!git clone https://www.github.com/GEM-benchmark/NL-Augmenter"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NL-Augmenter'...\n",
            "warning: redirecting to https://github.com/GEM-benchmark/NL-Augmenter.git/\n",
            "remote: Enumerating objects: 1195, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 1195 (delta 70), reused 65 (delta 55), pack-reused 1059\u001b[K\n",
            "Receiving objects: 100% (1195/1195), 225.09 KiB | 4.50 MiB/s, done.\n",
            "Resolving deltas: 100% (716/716), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9KCH1qpHjDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10795fe6-5bf2-4b74-f3f4-c81ba835aa12"
      },
      "source": [
        "cd NL-Augmenter"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NL-Augmenter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMiID0kSE_Qf"
      },
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkdXQrzKR0zY"
      },
      "source": [
        "## Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJcZGbR7JVFt"
      },
      "source": [
        "from transformations.butter_fingers_perturbation.transformation import ButterFingersPerturbation\n",
        "from transformations.change_person_named_entities.transformation import ChangePersonNamedEntities\n",
        "from transformations.replace_numerical_values.transformation import ReplaceNumericalValues\n",
        "from interfaces.SentenceOperation import SentenceOperation\n",
        "from interfaces.QuestionAnswerOperation import QuestionAnswerOperation\n",
        "from evaluation.evaluation_engine import evaluate, execute_model\n",
        "from tasks.TaskTypes import TaskType"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT8V407QBFYz"
      },
      "source": [
        "## Play with some existing transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MZfjp0toJdHh",
        "outputId": "829424e9-00e3-4036-e252-40a963e9e32c"
      },
      "source": [
        "t1 = ButterFingersPerturbation()\n",
        "t1.generate(\"Jason wants to move back to India by the end of next year.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Jasln wants to move back to India by the end od next year.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S_o9ktK9JwKs",
        "outputId": "26b9afd4-43be-4123-c78a-dad1089755a8"
      },
      "source": [
        "t2 = ChangePersonNamedEntities()\n",
        "t2.generate(\"Jason wants to move back to India by the end of next year.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Austin wants to move back to India by the end of next year.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v1khspY1AH_j",
        "outputId": "a2ecf7e8-31cf-4213-d935-f0ab00d1e9db"
      },
      "source": [
        "t3 = ReplaceNumericalValues()\n",
        "t3.generate(\"Jason's 3 sisters want to move back to India\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Jason's 8 sisters want to move back to India\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2CB0LRbBWST"
      },
      "source": [
        "## Define a simple transformation\n",
        "Let's define a very basic transformation which just uppercases the sentence. \n",
        "\n",
        "This transformation could be used for many [tasks](https://github.com/GEM-benchmark/NL-Augmenter/blob/add_filters_for_contrast_sets/tasks/TaskTypes.py) including text classification and generation. So, we need to populate the `tasks` variable to `[TaskType.TEXT_CLASSIFICATION, TaskType.TEXT_TO_TEXT_GENERATION]`. That's it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BymdwQ3PJzg7"
      },
      "source": [
        "class MySimpleTransformation(SentenceOperation):\n",
        "  tasks = [TaskType.TEXT_CLASSIFICATION, TaskType.TEXT_TO_TEXT_GENERATION]\n",
        "  locales = [\"en\"]\n",
        "  \n",
        "  def generate(self, sentence):\n",
        "    return sentence.upper()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkqtwqYlUWXV"
      },
      "source": [
        "my_transformation = MySimpleTransformation() "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rbbSJxJ8UbVz",
        "outputId": "522f7090-08ea-4976-8772-65766fe6d02b"
      },
      "source": [
        "my_transformation.generate(\"John was n't the person I had n't imagined.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"JOHN WAS N'T THE PERSON I HAD N'T IMAGINED.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8682Ql9GOP0"
      },
      "source": [
        "\n",
        "Obviously this can barely be called a transformation. What could this really achieve? Duh. \n",
        "So, let's quickly compare the performance of a trained text classifier on a common test set, and a test set with MySimpleTransformation applied (or also called as a pertubed set) with this one line of code. And you need to hold your breadth for around 5 minutes!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJlW0WnrVU0n"
      },
      "source": [
        "execute_model(MySimpleTransformation, \"TEXT_CLASSIFICATION\", percentage_of_examples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_NeVxa0RKWx"
      },
      "source": [
        "### üï∫ Voila! The accuracy on the perturbed set has fallen by 6% with this simple transformation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qmvF6sQRWu"
      },
      "source": [
        "So what happened internally? --> `execute_model` depending on the transformation type [SentenceOperation](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/interfaces/SentenceOperation.py)) and the task you provided (TEXT_CLASSIFICATION) evaluated a pre-trained model of HuggingFace. In this case, a sentiment analysis model [aychang/roberta-base-imdb](https://huggingface.co/aychang/roberta-base-imdb) was chosen and evaluated on 1% of the [IMDB dataset](https://huggingface.co/datasets/imdb) with and without the transformation to check if the sentiment is predicted correctly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmOP8B3-TW0i"
      },
      "source": [
        "If you want to evaluate this on your own model and dataset, you can pass the parameters as shown below in the `execute_model` method. Note that we obviously can't support each and every model type and dataset type and hence some models and datasets might require refactoring in the `evaluation_engine` class from your side and we are happy to help. üòä"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKcUmvYzYKAJ"
      },
      "source": [
        "# Here are the different parameters which are used as defaults!\n",
        "# execute_model(MySimpleTransformation, \"TEXT_CLASSIFICATION\", \"en\", model_name = \"aychang/roberta-base-imdb\", dataset=\"imdb\", percentage_of_examples=1)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHuYXB6OWNiU"
      },
      "source": [
        "##  A Model Based Transformation\n",
        "We don't want to restrict ourselves with just string level changes! We want to do more, don't we? So, let's use a pre-trained paraphrase generator to transform question answering examples. There is an exisiting interface [QuestionAnswerOperation](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/interfaces/QuestionAnswerOperation.py) which takes as input the context, the question and the answer as inputs. Let's use that to augment our training data for question answering! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3DehjWXYwnn"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "class MySecondTransformation(QuestionAnswerOperation):\n",
        "  tasks = [TaskType.QUESTION_ANSWERING, TaskType.QUESTION_GENERATION]\n",
        "  locales = [\"en\"]\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model_name=\"prithivida/parrot_paraphraser_on_T5\"\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "  def generate(self, context, question, answers): # Note that the choice of inputs for 'generate' is consistent with those in QuestionAnswerOperation\n",
        "    \n",
        "    # Let's call the HF model to generate a paraphrase for the question\n",
        "    paraphrase_input = question\n",
        "    batch = self.tokenizer([paraphrase_input],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\")\n",
        "    translated = self.model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=1, temperature=1.5)\n",
        "    paraphrased_question = self.tokenizer.batch_decode(translated, skip_special_tokens=True) \n",
        "\n",
        "    # context = \"Apply your own logic here\"\n",
        "    # answers = \"And here too :)\"\n",
        "\n",
        "    # return the new question-answering example\n",
        "    return context, paraphrased_question, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84G9YzdGblfP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0ebe94b3466943ddba5b8ab874daf900",
            "a905fb867c8a44338d5f35f38a19b46e",
            "14bb03f053f64a278e3968840eb959fd",
            "0f6a1bf4be7b42fd9c0c3929fd478e3b",
            "962f4cd8298940368632450846bca967",
            "90f2d6ac1f17408a9408d6c316a1e29e",
            "4958a0e991d049339723fe494f573229",
            "8933b6543ead40fb826e135d82f5982c"
          ]
        },
        "outputId": "990d75ce-e75d-4a65-baff-9400b6a38955"
      },
      "source": [
        "t4 = MySecondTransformation()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ebe94b3466943ddba5b8ab874daf900",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891737400.0, style=ProgressStyle(descri‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFY0lGA2lIqy",
        "outputId": "24b002be-c91a-4768-c24e-e16d54b24fb8"
      },
      "source": [
        "t4.generate(context=\"Mumbai, Bengaluru, New Delhi are among the many famous places in India.\", \n",
        "            question=\"What are the famous places we should not miss in India?\", \n",
        "            answers=[\"Mumbai\", \"Bengaluru\", \"Delhi\", \"New Delhi\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Mumbai, Bengaluru, New Delhi are among the many famous places in India.',\n",
              " ['recommend some of the best places to visit in India?'],\n",
              " ['Mumbai', 'Bengaluru', 'Delhi', 'New Delhi'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOCoNfaV6F9l"
      },
      "source": [
        "Voila! Seems like you have created a new training example now for question-answering and question-generation! üéâ üéä üéâ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WF-JtPd6wAm"
      },
      "source": [
        "#Now you are all ready to contribute a transformation to [NL-Augmenter ü¶é ‚Üí üêç](https://github.com/GEM-benchmark/NL-Augmenter)! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e9Y57h0wh-"
      },
      "source": [
        "## What is this deal with filters?\n",
        "So, just the way transformations can transform examples of text, filters can identify whether an example follows some pattern of text! The only difference is that while transformations return another example of the same input format, filters return True or False!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDpDvOjv2Yx9"
      },
      "source": [
        "sentence --> SentenceOperation.**generate**(sentence) --> another-sentence\n",
        "\n",
        "sentence --> SentenceOperation.**filter**(sentence)  --> TRUE/FALSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChHoHeq8CGXX"
      },
      "source": [
        "#So, let's play with some existing filters! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfUvpkSN0BKB"
      },
      "source": [
        "from filters.keywords import TextContainsKeywordsFilter\n",
        "from filters.length import TextLengthFilter, SentenceAndTargetLengthFilter"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le0p5dsBDGA1"
      },
      "source": [
        "The `TextLengthFilter` accepts an input sentence if the length of the input sentence is within the initialised range. Let's initialise this filter to accept all sentences with length greater than 10 tokens!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb2u3gsE0d_n"
      },
      "source": [
        "f1 = TextLengthFilter(\">\", 10)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK0xTRBsFCdQ",
        "outputId": "24aebfc2-4d58-4278-a049-cdf3d235ec0e"
      },
      "source": [
        "f1.filter(\"This sentence is long enough to pass while you think of implementing your own filter!\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diGI4EaOFSun",
        "outputId": "e4f3205d-abbf-4483-e37f-7126410531c6"
      },
      "source": [
        "f1.filter(\"This one's too short!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZMYN4COFiY8"
      },
      "source": [
        "Let's say you have a lot of paraphrasing data and you intend to train a paraphrase generator to convert longer sentences to shorter ones! Check how the `SentenceAndTargetLengthFilter` can be used for this!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H27VEe8pFYMl"
      },
      "source": [
        "f2 = SentenceAndTargetLengthFilter([\">\", \"<\"], [10,8])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql1ZSsyjG_Y7",
        "outputId": "066fd81f-ac9f-400d-d14d-be26dabdc84b"
      },
      "source": [
        "f2.filter(\"That show is going to take place in front of immensely massive crowds.\", \n",
        "          \"Large crowds would attend the show.\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKkFIgAtHsB-",
        "outputId": "5f17c054-a00f-4aa2-dc7a-b19b4e719a0d"
      },
      "source": [
        "f2.filter(\"The film was nominated for the Academy Award for Best Art Direction.\", \n",
        "          \"The movie was a nominee for the Academy Award for Best Art Direction.\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAScGDfaJKa9"
      },
      "source": [
        "Okay, now that you've said to yourself that these filters are too basic, let's try to make a simple and interesting one! \n",
        "\n",
        "Let's define a filter which selects question-answer pairs which share a low lexical overlap between the question and the context!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7b1mg2ZJcsc"
      },
      "source": [
        "import spacy\n",
        "\n",
        "class LowLexicalOverlapFilter(QuestionAnswerOperation):\n",
        "  tasks = [TaskType.QUESTION_ANSWERING, TaskType.QUESTION_GENERATION]\n",
        "  locales = [\"en\"]\n",
        "  \n",
        "  def __init__(self, threshold=3):\n",
        "    super().__init__()\n",
        "    self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def filter(self, context, question, answers): \n",
        "    # Note that the only difference between a filter and a transformation is this method! \n",
        "    # The inputs remain the same!\n",
        "    \n",
        "    question_tokenized = self.nlp(question, disable=[\"parser\", \"tagger\", \"ner\"])\n",
        "    context_tokenized = self.nlp(context, disable=[\"parser\", \"tagger\", \"ner\"])\n",
        "    \n",
        "    q_tokens = set([t.text for t in question_tokenized])\n",
        "    c_tokens = set([t.text for t in context_tokenized])\n",
        "    \n",
        "    low_lexical_overlap = len(q_tokens.intersection(c_tokens)) > self.threshold\n",
        "    return low_lexical_overlap"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtKYvAr2MbSf"
      },
      "source": [
        "f3 = LowLexicalOverlapFilter()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1np6KirQGZc",
        "outputId": "3cf70a61-266d-4926-c1a5-abae8be318c2"
      },
      "source": [
        "f3.filter(\"New York, is the most populous city in the United States.\",\n",
        "          \"Which is the most populous city of the United States?\",\n",
        "          [\"New York\"])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbFOyYHUQVnk",
        "outputId": "d375bde0-6b3d-4717-df21-2c3c46be1a69"
      },
      "source": [
        "f3.filter(\"New York, is the most populous city in the United States.\",\n",
        "          \"Which city has the largest population in the US?\",\n",
        "          [\"New York\"])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0NY7NKRGfE"
      },
      "source": [
        "That's it!  So you have created a new filter which can separate the hard examples from the easy one! üéâ üéä üéâ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHd2QC_RkwB"
      },
      "source": [
        "#Now go ahead and contribute a nice filter to [NL-Augmenter ü¶é ‚Üí üêç](https://github.com/GEM-benchmark/NL-Augmenter)! "
      ]
    }
  ]
}
